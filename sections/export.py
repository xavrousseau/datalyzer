# ============================================================
# Fichier : sections/export.py
# Objectif : Export des donn√©es avec s√©lection de colonnes ET de lignes,
#            format, logs, snapshot et t√©l√©chargement
# Version  : enrichie (filtres ET/OU, √©chantillon, top-N, d√©dup, NA)
# Auteur   : Xavier Rousseau ‚Äî commentaires & docs p√©dagogiques ajout√©s
# ============================================================

from __future__ import annotations

import os
import re
import time
from typing import Tuple, Optional, List

import numpy as np
import pandas as pd
import streamlit as st

from utils.snapshot_utils import save_snapshot
from utils.log_utils import log_action
from utils.filters import get_active_dataframe
from utils.ui_utils import section_header, show_footer  # ‚Üê en-t√™te/pied unifi√©s

# =============================================================================
# Helpers ‚Äî petites fonctions utilitaires, pures et testables
# =============================================================================

def _sanitize_filename(name: str) -> str:
    """Nettoie un nom brut pour produire un nom de fichier ¬´ s√ªr ¬ª.

    R√®gles appliqu√©es :
      - Trim des espaces, remplacement des caract√®res non alphanum√©riques par ¬´ _ ¬ª
      - Conservation uniquement de `[A-Za-z0-9_-.]`
      - Compression des underscores cons√©cutifs, suppression des `_`/`.` en bord
      - Valeur de repli : "export" si tout est vide

    Pourquoi :
      - √âviter les caract√®res probl√©matiques pour diff√©rents OS/navigateurs
      - Garantir un nom stable pour l'√©criture disque et le bouton de DL

    Args:
        name: Nom saisi par l'utilisateur (peut √™tre vide ou bruit√©)

    Returns:
        Nom assaini pr√™t √† √™tre joint √† une extension
    """
    name = (name or "").strip()
    name = re.sub(r"[^\w\-.]+", "_", name)
    name = re.sub(r"_+", "_", name).strip("_.")
    return name or "export"


def _ensure_extension(filename: str, file_format: str) -> str:
    """Force l'extension coh√©rente avec le format demand√©.

    Exemples :
      - ("report", "csv")   ‚Üí "report.csv"
      - ("data.JSON", "json") ‚Üí "data.JSON" (d√©j√† OK, insensible √† la casse)

    Args:
        filename: Nom de base (avec ou sans extension)
        file_format: Format cible parmi {csv, xlsx, json, parquet}

    Returns:
        Nom termin√© par l'extension correcte
    """
    ext = file_format.lower()
    if not filename.lower().endswith(f".{ext}"):
        return f"{filename}.{ext}"
    return filename


def _mime_for(fmt: str) -> str:
    """Retourne le type MIME attendu par `st.download_button`.

    Remarque : Parquet n'a pas un unique MIME officiel universellement reconnu,
    on utilise un binaire g√©n√©rique.
    """
    return {
        "csv": "text/csv",
        "xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "json": "application/json",
        "parquet": "application/octet-stream",
    }.get(fmt, "application/octet-stream")


def _dtype_kind(s: pd.Series) -> str:
    """D√©tecte une ¬´ famille ¬ª de type de donn√©es pour piloter l'UI et les op√©rateurs.

    Cat√©gories retourn√©es :
      - "numeric"  : colonnes num√©riques (int/float/decimal)
      - "datetime" : colonnes datetime64 (toutes variantes)
      - "bool"     : bool√©ens
      - "text"     : fallback (objets/strings)

    Args:
        s: S√©rie pandas dont on veut conna√Ætre la famille de type

    Returns:
        Nom de la famille de type (voir ci-dessus)
    """
    if pd.api.types.is_bool_dtype(s):
        return "bool"
    if pd.api.types.is_numeric_dtype(s):
        return "numeric"
    if pd.api.types.is_datetime64_any_dtype(s):
        return "datetime"
    return "text"


def _build_rule_mask(
    df: pd.DataFrame,
    col: str,
    op: str,
    v1: Optional[str],
    v2: Optional[str],
) -> pd.Series:
    """Construit un masque bool√©en pour une r√®gle de filtrage (sans `.query`).

    La logique s'adapte dynamiquement au type de la colonne et √† l'op√©rateur.
    Les valeurs v1/v2 proviennent des widgets Streamlit (donc strings et potentiels
    NaN/vides) ; elles sont *cast√©es* de mani√®re tol√©rante.

    Exemple d'usage :
        mask = _build_rule_mask(df, "age", ">=", "30", None)
        df_filt = df[mask]

    Args:
        df: DataFrame source
        col: Nom de la colonne cibl√©e
        op: Op√©rateur choisi (ex: '==', 'between', 'contains', ...)
        v1: Premi√®re valeur (ou None/"" si non applicable)
        v2: Seconde valeur (pour 'between' notamment)

    Returns:
        pd.Series bool√©enne index√©e comme `df` (True = ligne conserveÃÅe)
    """
    s = df[col]
    kind = _dtype_kind(s)

    # ‚Äî Cas universels : tests de nullit√© ‚Äî
    if op == "is null":
        return s.isna()
    if op == "is not null":
        return s.notna()

    # ‚Äî Casting des entr√©es selon le type d√©tect√© ‚Äî
    if kind == "numeric":
        def _to_num(x):
            try:
                return float(x)
            except Exception:
                return np.nan
        a = _to_num(v1)
        b = _to_num(v2) if v2 is not None else np.nan
    elif kind == "datetime":
        a = pd.to_datetime(v1, errors="coerce") if v1 is not None else pd.NaT
        b = pd.to_datetime(v2, errors="coerce") if v2 is not None else pd.NaT
    elif kind == "bool":
        # On tol√®re plusieurs √©critures (fr/en/num√©riques)
        map_bool = {"true": True, "false": False, "1": True, "0": False, "vrai": True, "faux": False}
        val = (v1 or "").strip().lower()
        a = map_bool.get(val, None)
    else:  # text
        a = "" if v1 is None else str(v1)
        b = "" if v2 is None else str(v2)

    # ‚Äî Op√©rateurs contextualis√©s par type ‚Äî
    if kind == "numeric":
        if op == "==": return s == a
        if op == "!=": return s != a
        if op == "<":  return s < a
        if op == "<=": return s <= a
        if op == ">":  return s > a
        if op == ">=": return s >= a
        if op == "between":
            return s.between(a, b, inclusive="both")

    elif kind == "datetime":
        if op == "==": return s == a
        if op == "!=": return s != a
        if op == "before": return s < a
        if op == "after":  return s > a
        if op == "between":
            return s.between(a, b, inclusive="both")

    elif kind == "bool":
        # Valeur bool√©enne non reconnue ‚Üí masque False partout (fail-safe)
        if a is None:
            return pd.Series(False, index=s.index)
        if op == "==": return s == a
        if op == "!=": return s != a

    else:  # text
        # Pour le texte, on op√®re sur une vue string (g√®re NaN proprement)
        sv = s.astype("string")
        if op == "==": return sv.fillna("") == a
        if op == "!=": return sv.fillna("") != a
        if op == "contains":     return sv.fillna("").str.contains(a, case=False, na=False)
        if op == "not contains": return ~sv.fillna("").str.contains(a, case=False, na=False)
        if op == "startswith":   return sv.fillna("").str.startswith(a, na=False)
        if op == "endswith":     return sv.fillna("").str.endswith(a, na=False)

    # ‚Äî Op√©rateur non g√©r√© : par s√©curit√©, on exclut tout ‚Äî
    return pd.Series(False, index=s.index)


def _operators_for(s: pd.Series) -> List[str]:
    """Retourne la liste d'op√©rateurs pertinents pour une colonne donn√©e.

    L'UI peut utiliser ce r√©sultat pour limiter les choix de l'utilisateur
    √† des combinaisons coh√©rentes (ex: 'between' pour num√©riques/dates, etc.).
    """
    kind = _dtype_kind(s)
    if kind == "numeric":
        return ["==", "!=", "<", "<=", ">", ">=", "between", "is null", "is not null"]
    if kind == "datetime":
        return ["==", "!=", "before", "after", "between", "is null", "is not null"]
    if kind == "bool":
        return ["==", "!=", "is null", "is not null"]
    return ["==", "!=", "contains", "not contains", "startswith", "endswith", "is null", "is not null"]


# =============================================================================
# Vue principale ‚Äî orchestratrice de la page d'export
# =============================================================================

def run_export() -> None:
    """Affiche la page ¬´ Export du fichier final ¬ª et orchestre l'export.

    La fonction suit un pipeline *lisible* :
      1) Contexte & en-t√™te
      2) S√©lection des colonnes √† inclure
      3) S√©lection des lignes (tout / r√®gles AND-OR / √©chantillon / Top-N tri√©)
      4) Nettoyage optionnel (dropna, d√©duplication)
      5) Pr√©visualisation rapide du r√©sultat
      6) Param√©trage du format (CSV/XLSX/JSON/Parquet), encodage et compression
      7) √âcriture disque + bouton de t√©l√©chargement + log + snapshot

    Design d'UX :
      - Messages clairs (compteurs de lignes/colonnes, avertissements)
      - Valeurs par d√©faut s√ªres, widgets guid√©s par type de donn√©es
      - √âvitement de `.query` pour garder le typage explicite et la robustesse
    """
    # ---------- 1) En-t√™te unifi√© ----------
    section_header(
        title="Export du fichier final",
        subtitle="Choisissez les colonnes, les lignes, le format ‚Äî puis t√©l√©chargez.",
        section="export",
        emoji="",
    )

    # ---------- 2) DF actif ----------
    df, nom = get_active_dataframe()
    if df is None or df.empty:
        st.warning("‚ùå Aucune donn√©e disponible pour l‚Äôexport.")
        return

    st.markdown(f"üîé **Fichier actif : `{nom}`** ‚Äî **{df.shape[0]}** lignes √ó **{df.shape[1]}** colonnes")

    # ---------- 3) Colonnes √† inclure ----------
    st.subheader("üß© Colonnes √† inclure")
    selected_columns = st.multiselect(
        "S√©lectionnez les colonnes √† exporter",
        options=df.columns.tolist(),
        default=df.columns.tolist(),
        help="D√©cochez les colonnes inutiles pour un export plus l√©ger."
    )
    st.write(f"üî¢ **{len(selected_columns)} colonne(s) s√©lectionn√©e(s)**")
    if not selected_columns:
        st.info("S√©lectionnez au moins une colonne pour poursuivre.")
        return

    # ---------- 4) Lignes √† exporter ----------
    st.subheader("üéöÔ∏è Lignes √† exporter")
    mode = st.radio(
        "Source des lignes",
        ["Toutes les lignes", "Filtrer avec des r√®gles", "√âchantillon al√©atoire", "Top N tri√©"],
        horizontal=True,
    )

    # Par d√©faut : on emporte tout, puis on sp√©cialise selon le mode
    df_rows = df

    # ---- 4.a) Filtrer avec des r√®gles (AND/OR) ----
    if mode == "Filtrer avec des r√®gles":
        combi = st.radio("Combinaison des r√®gles", ["ET", "OU"], horizontal=True)
        nb_rules = st.number_input("Nombre de r√®gles", min_value=1, max_value=5, value=1, step=1)

        masks: List[pd.Series] = []
        for i in range(int(nb_rules)):
            st.markdown(f"**R√®gle {i+1}**")

            # 1) Choix de la colonne
            col = st.selectbox(
                f"Colonne #{i+1}",
                options=df.columns.tolist(),
                key=f"rule_col_{i}",
            )

            # 2) Op√©rateur contextualis√© par type
            ops = _operators_for(df[col])
            op = st.selectbox(
                f"Op√©rateur #{i+1}",
                options=ops,
                key=f"rule_op_{i}",
            )

            # 3) Widgets de saisie adapt√©s √† l'op√©rateur et au type
            v1 = v2 = None
            kind = _dtype_kind(df[col])
            if op in {"is null", "is not null"}:
                # Aucun param√®tre requis
                pass
            elif op == "between":
                if kind == "datetime":
                    # S√©lecteurs de dates Streamlit ‚Üí Timestamp pandas
                    d1 = st.date_input(f"Valeur min #{i+1}")
                    d2 = st.date_input(f"Valeur max #{i+1}")
                    v1 = pd.Timestamp(d1)
                    v2 = pd.Timestamp(d2)
                elif kind == "numeric":
                    c1, c2 = st.columns(2)
                    v1 = c1.text_input(f"Min #{i+1}", value="")
                    v2 = c2.text_input(f"Max #{i+1}", value="")
                else:
                    # Cas texte : interpr√©t√© comme bornes lexicales
                    v1 = st.text_input(f"Min #{i+1}", value="")
                    v2 = st.text_input(f"Max #{i+1}", value="")
            else:
                # Op√©rateurs √† une seule valeur
                if kind == "datetime":
                    d = st.date_input(f"Valeur #{i+1}")
                    v1 = pd.Timestamp(d)
                elif kind == "bool":
                    v1 = st.selectbox(
                        f"Valeur #{i+1}",
                        options=["true", "false", "1", "0", "vrai", "faux"],
                        index=0,
                    )
                elif kind == "numeric":
                    v1 = st.text_input(f"Valeur #{i+1}", value="")
                else:
                    v1 = st.text_input(f"Valeur #{i+1}", value="")

            # 4) Construction robuste du masque (avec garde-fous)
            try:
                mask = _build_rule_mask(
                    df,
                    col,
                    op,
                    None if v1 == "" else v1,
                    None if v2 == "" else v2,
                )
            except Exception as e:
                st.error(f"R√®gle {i+1} invalide : {e}")
                mask = pd.Series(False, index=df.index)
            masks.append(mask)

        # 5) Agr√©gation ET/OU des masques
        if masks:
            if combi == "ET":
                final_mask = masks[0]
                for m in masks[1:]:
                    final_mask = final_mask & m
            else:  # OU
                final_mask = masks[0]
                for m in masks[1:]:
                    final_mask = final_mask | m
            df_rows = df[final_mask]

        st.info(f"üîé Lignes correspondantes : **{len(df_rows)}** / {len(df)}")

    # ---- 4.b) √âchantillon al√©atoire ----
    if mode == "√âchantillon al√©atoire":
        c1, c2 = st.columns(2)
        n = c1.number_input(
            "Taille de l‚Äô√©chantillon (n)",
            min_value=1,
            max_value=max(1, len(df)),
            value=min(1000, len(df)),
        )
        seed = c2.number_input("Graine al√©atoire (seed)", min_value=0, max_value=999999, value=42)

        if n > len(df):
            st.warning("La taille demand√©e d√©passe la taille du dataset : on utilisera tout le dataset.")
            n = len(df)
        df_rows = df.sample(n=int(n), random_state=int(seed))

    # ---- 4.c) Top N tri√© ----
    if mode == "Top N tri√©":
        sort_col = st.selectbox("Trier par", options=df.columns.tolist())
        ascending = st.toggle("Tri croissant", value=False)
        n = st.number_input("Nombre de lignes (Top N)", min_value=1, max_value=max(1, len(df)), value=min(1000, len(df)))

        # Optionnel : on ignore les NaN pour un tri plus lisible
        dropna_sort = st.checkbox("Ignorer les NaN pour le tri", value=True)
        tmp = df if not dropna_sort else df[df[sort_col].notna()]
        df_rows = tmp.sort_values(by=sort_col, ascending=ascending).head(int(n))

    # ---------- 5) Nettoyage optionnel des lignes s√©lectionn√©es ----------
    st.subheader("üßπ Nettoyage optionnel des lignes s√©lectionn√©es")
    c1, c2, c3 = st.columns(3)
    dedup_on = c1.checkbox(
        "Supprimer les doublons",
        value=False,
        help="Supprime les lignes dupliqu√©es sur un sous-ensemble de colonnes."
    )
    dropna_rows = c2.checkbox("Supprimer les lignes avec NA (colonnes choisies)", value=False)
    keep_first = c3.radio(
        "Conserver en cas de doublons",
        ["first", "last"],
        horizontal=True,
        index=0,
        disabled=not dedup_on,
    )

    dedup_subset: List[str] = []
    if dedup_on:
        dedup_subset = st.multiselect(
            "Colonnes utilis√©es pour la d√©duplication",
            options=selected_columns,
            default=selected_columns,
            help="D√©finissez la notion de doublon (par d√©faut : m√™mes valeurs sur toutes les colonnes export√©es).",
        )

    # Application effective du nettoyage, sur une copie (immutabilit√© soft)
    df_rows_clean = df_rows.copy()
    if dropna_rows:
        df_rows_clean = df_rows_clean.dropna(subset=selected_columns, how="any")
    if dedup_on:
        df_rows_clean = df_rows_clean.drop_duplicates(subset=dedup_subset or None, keep=keep_first)

    # ---------- 6) Aper√ßu (post-filtres/tri/√©chantillon/d√©dup) ----------
    with st.expander("üîç Aper√ßu du r√©sultat (apr√®s filtres/tri/√©chantillon/d√©dup)", expanded=False):
        st.dataframe(df_rows_clean[selected_columns].head(50), use_container_width=True)
    st.caption(f"R√©sultat courant : **{len(df_rows_clean)}** lignes √ó **{len(selected_columns)}** colonnes")

    # ---------- 7) Options de base d'export ----------
    include_index = st.checkbox("Inclure l‚Äôindex dans le fichier export√©", value=False)

    st.subheader("üì¶ Format du fichier")
    file_format = st.selectbox("Format", options=["csv", "xlsx", "json", "parquet"], index=0)

    col_enc, col_comp = st.columns(2)
    with col_enc:
        encoding = (
            st.selectbox(
                "Encodage (CSV/JSON)",
                options=["utf-8", "utf-8-sig", "latin-1"],
                index=0,
                help="UTF-8 recommand√©. 'utf-8-sig' utile pour Excel ancien ; 'latin-1' seulement si outil incompatible UTF-8.",
            )
            if file_format in {"csv", "json"}
            else "utf-8"
        )
    with col_comp:
        compression = (
            st.selectbox(
                "Compression",
                options=["aucune", "gzip"],
                index=0,
                help="CSV/JSON/Parquet supportent gzip. XLSX est d√©j√† compress√©.",
            )
            if file_format in {"csv", "json", "parquet"}
            else "aucune"
        )

    # ---------- 8) Nom de fichier ----------
    ts = int(time.time())
    default_base = _sanitize_filename(f"export_final_{ts}")
    file_name_input = st.text_input(
        "Nom du fichier export√© (sans extension ou avec extension correspondante)",
        value=default_base,
    )
    file_name_input = _sanitize_filename(file_name_input)
    file_name = _ensure_extension(file_name_input, file_format)

    # ---------- 9) Action d'export ----------
    st.subheader("‚¨áÔ∏è G√©n√©ration du fichier")
    if st.button("üì• G√©n√©rer et t√©l√©charger le fichier", type="primary"):
        try:
            # (1) Sous-ensemble final (lignes + colonnes)
            df_export = df_rows_clean[selected_columns]

            # (2) R√©pertoire cible persistant
            export_dir = os.path.join("data", "exports")
            os.makedirs(export_dir, exist_ok=True)  # idempotent
            export_path = os.path.join(export_dir, file_name)

            # (3) √âcriture selon format
            if file_format == "csv":
                comp = "gzip" if compression == "gzip" else None
                df_export.to_csv(
                    export_path,
                    index=include_index,
                    encoding=encoding,
                    lineterminator="\n",
                    compression=comp,
                )

            elif file_format == "xlsx":
                # XLSX est d√©j√† compress√© (ZIP) ‚Üí pas de gzip par-dessus
                df_export.to_excel(export_path, index=include_index, engine="openpyxl")

            elif file_format == "json":
                comp = "gzip" if compression == "gzip" else None
                df_export.to_json(
                    export_path,
                    orient="records",
                    force_ascii=False,  # respecte l'UTF-8
                    compression=comp,
                )

            elif file_format == "parquet":
                comp = "gzip" if compression == "gzip" else None
                df_export.to_parquet(export_path, index=include_index, compression=comp)

            else:
                # En th√©orie inaccessible car l'UI borne les choix
                raise ValueError(f"Format non g√©r√© : {file_format}")

            # (4) Log + snapshot ‚Äî utile pour audit et MLOps light
            save_snapshot(df_export, suffix=file_format)
            log_action(
                "export",
                f"{file_name} ‚Äî rows={len(df_export)} ‚Äî cols={len(selected_columns)} ‚Äî "
                f"mode={mode} ‚Äî format={file_format} ‚Äî comp={compression}",
            )

            # (5) Bouton de t√©l√©chargement (lecture binaire du fichier √©crit)
            with open(export_path, "rb") as f:
                payload = f.read()

            st.success(f"‚úÖ Fichier export√© : **{file_name}**")
            st.download_button(
                label="üì• T√©l√©charger maintenant",
                data=payload,
                file_name=file_name,
                mime=_mime_for(file_format),
            )
            st.caption(f"Fichier enregistr√© sur disque : `{export_path}`")

        except Exception as e:
            # Remont√©e d'erreur lisible pour l'utilisateur final
            st.error(f"‚ùå Erreur pendant l‚Äôexport : {e}")

    # ---------- 10) Footer ----------
    show_footer(
        author="Xavier Rousseau",
        site_url="https://xavrousseau.github.io/",
        version="1.0",
    )
