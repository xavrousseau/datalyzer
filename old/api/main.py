# =============================================================================
# IMPORTATION DES LIBRAIRIES
# =============================================================================

# FastAPI : Framework permettant de cr√©er des API web rapides et performantes.
from fastapi import FastAPI, UploadFile, File

# fastapi.responses fournit des classes de r√©ponses HTTP pour renvoyer du JSON ou des fichiers.
from fastapi.responses import JSONResponse, FileResponse

# Pydantic : biblioth√®que pour la validation des donn√©es via des mod√®les (ici utilis√© pour le mod√®le de suppression de colonnes).
from pydantic import BaseModel

# typing.List permet d'indiquer le type d'une liste dans les annotations.
from typing import List

# pandas est utilis√© pour manipuler et analyser des donn√©es sous forme de DataFrame.
import pandas as pd

# numpy est utilis√© pour les op√©rations num√©riques et math√©matiques.
import numpy as np

# os permet d'interagir avec le syst√®me de fichiers (cr√©ation de dossiers, chemin d'acc√®s, etc.).
import os

# io permet de traiter des flux de donn√©es (par exemple, convertir des donn√©es binaires en format lisible par pandas).
import io


# =============================================================================
# ‚öôÔ∏è CONFIGURATION DE L'API
# =============================================================================

# Cr√©ation de l'application FastAPI avec un titre personnalis√©.
app = FastAPI(title="EDA Backend API")

# D√©finition du r√©pertoire o√π seront stock√©s les fichiers upload√©s.
UPLOAD_DIR = "data/uploads"
# Cr√©e le dossier "data/uploads" s'il n'existe pas d√©j√†.
os.makedirs(UPLOAD_DIR, exist_ok=True)

# Stockage temporaire en m√©moire du DataFrame charg√©.
# Ce dictionnaire servira de cache pour stocker les donn√©es durant la session.
DATA_STORE = {"df": None}


# =============================================================================
# üîß UTILITAIRE : NETTOYAGE POUR JSON
# =============================================================================
def make_json_safe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remplace toutes les valeurs non compatibles JSON par None.
    
    Certains types de valeurs (comme NaN, inf, -inf, NA, NaT) ne peuvent pas √™tre 
    s√©rialis√©s en JSON. Cette fonction remplace ces valeurs par None afin de garantir 
    la compatibilit√© avec JSON.
    
    Args:
        df (pd.DataFrame): Le DataFrame √† nettoyer.
    
    Returns:
        pd.DataFrame: Le DataFrame avec les valeurs incompatibles remplac√©es par None.
    """
    df = df.replace({
        float('inf'): None,
        float('-inf'): None,
        pd.NaT: None,
        pd.NA: None,
        np.nan: None
    })
    # La m√©thode where remplace toutes les valeurs non-nulles par elles-m√™mes, et les valeurs nulles par None.
    return df.where(pd.notnull(df), None)


# =============================================================================
# üåê ENDPOINT RACINE
# =============================================================================
@app.get("/")
def root():
    """
    Endpoint racine de l'API.
    
    Retourne un message de bienvenue, la liste des endpoints disponibles 
    et l'URL de la documentation interactive (/docs).
    """
    return {
        "message": "üéâ Bienvenue sur l'API EDA Explorer !",
        "endpoints_disponibles": [
            "/upload/",
            "/head/",
            "/missing-values/",
            "/describe/",
            "/duplicates/",
            "/drop-duplicates/",
            "/drop-columns/",
            "/export/",
            "/report/"
        ],
        "docs": "/docs"
    }


# =============================================================================
# üì¶ MOD√àLE PYDANTIC POUR LA SUPPRESSION DE COLONNES
# =============================================================================
class ColumnDropRequest(BaseModel):
    """
    Mod√®le de donn√©es pour la requ√™te de suppression de colonnes.
    
    Attributes:
        columns (List[str]): Liste des noms de colonnes √† supprimer.
    """
    columns: List[str]


# =============================================================================
# üì§ UPLOAD D‚ÄôUN FICHIER CSV
# =============================================================================
@app.post("/upload/")
async def upload_csv(file: UploadFile = File(...)):
    """
    Endpoint pour uploader un fichier CSV.
    
    Processus :
    1. Lecture asynchrone du contenu du fichier upload√©.
    2. Conversion du contenu en DataFrame √† l'aide de pd.read_csv (via un flux BytesIO).
    3. Stockage du DataFrame dans DATA_STORE pour utilisation ult√©rieure.
    4. Retourne des m√©tadonn√©es sur le fichier (nom, colonnes et nombre de lignes).
    
    En cas d'erreur, renvoie une r√©ponse JSON avec le message d'erreur.
    """
    try:
        # Lecture du contenu du fichier en m√©moire (donn√©es binaires)
        contents = await file.read()
        # Conversion du contenu en DataFrame en utilisant BytesIO pour cr√©er un flux lisible par pandas.
        df = pd.read_csv(io.BytesIO(contents))
        # Stockage temporaire du DataFrame dans DATA_STORE.
        DATA_STORE["df"] = df
        # Retourne les m√©tadonn√©es du fichier upload√©.
        return {
            "filename": file.filename,
            "columns": df.columns.tolist(),
            "rows": len(df)
        }
    except Exception as e:
        # En cas d'erreur, renvoie une r√©ponse d'erreur d√©taill√©e.
        return JSONResponse(status_code=400, content={"error": str(e)})


# =============================================================================
# üëÄ AFFICHAGE DES PREMI√àRES LIGNES
# =============================================================================
@app.get("/head/")
def get_head(n: int = 5):
    """
    Endpoint pour r√©cup√©rer les 'n' premi√®res lignes du DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame a √©t√© charg√©.
    2. Extrait les premi√®res lignes avec la m√©thode head().
    3. Applique make_json_safe pour remplacer les valeurs incompatibles JSON par None.
    4. Retourne le r√©sultat sous forme d'une liste de dictionnaires.
    
    Args:
        n (int): Nombre de lignes √† retourner (par d√©faut 5).
    """
    df = DATA_STORE.get("df")
    if df is None:
        # Si aucun fichier n'est charg√©, renvoie un message d'erreur.
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # Extraction des n premi√®res lignes et nettoyage pour JSON.
    head_df = make_json_safe(df.head(n))
    return head_df.to_dict(orient="records")


# =============================================================================
# üßº D√âTAILS DES VALEURS MANQUANTES
# =============================================================================
@app.get("/missing-values/")
def get_missing_values():
    """
    Endpoint pour obtenir le d√©tail des valeurs manquantes dans le DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Calcule le nombre de valeurs manquantes par colonne.
    3. Calcule le pourcentage de valeurs manquantes pour chaque colonne.
    4. Retourne uniquement les colonnes pr√©sentant des valeurs manquantes.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    
    # Calcul du nombre de valeurs manquantes par colonne.
    missing = df.isnull().sum()
    # Calcul du pourcentage de valeurs manquantes par colonne.
    percent = (missing / len(df)) * 100
    # Cr√©ation d'un DataFrame r√©capitulatif.
    result = pd.DataFrame({
        "column": missing.index,
        "missing_count": missing.values,
        "missing_percent": percent.values
    })
    # Retourne uniquement les colonnes o√π le nombre de valeurs manquantes est sup√©rieur √† 0.
    return result[result["missing_count"] > 0].to_dict(orient="records")


# =============================================================================
# üìä STATISTIQUES DESCRIPTIVES
# =============================================================================
@app.get("/describe/")
def get_description():
    """
    Endpoint pour g√©n√©rer des statistiques descriptives du DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Calcule un r√©sum√© statistique en utilisant la m√©thode describe() de pandas.
    3. Nettoie le r√©sum√© via make_json_safe pour garantir la compatibilit√© JSON.
    4. Retourne le r√©sum√© sous forme de dictionnaire.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # Calcul du r√©sum√© descriptif et nettoyage pour JSON.
    desc = make_json_safe(df.describe(include="all"))
    return desc.to_dict()


# =============================================================================
# üì¶ D√âTECTION DE DOUBLONS
# =============================================================================
@app.get("/duplicates/")
def get_duplicates():
    """
    Endpoint pour d√©tecter les doublons dans le DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Utilise la m√©thode duplicated() pour identifier les lignes dupliqu√©es.
    3. Retourne le nombre de lignes dupliqu√©es et le pourcentage par rapport au total.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # Identification des doublons dans le DataFrame.
    dup = df[df.duplicated()]
    return {
        "duplicate_rows": len(dup),
        "percentage": round(100 * len(dup) / len(df), 2)
    }


# =============================================================================
# üßπ SUPPRESSION DES DOUBLONS
# =============================================================================
@app.post("/drop-duplicates/")
def drop_duplicates():
    """
    Endpoint pour supprimer les doublons du DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Supprime les doublons en r√©initialisant l'index.
    3. Met √† jour DATA_STORE avec le DataFrame nettoy√©.
    4. Retourne le nombre de doublons supprim√©s et le nombre de lignes restantes.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    before = len(df)  # Nombre de lignes avant suppression des doublons.
    df_cleaned = df.drop_duplicates().reset_index(drop=True)
    DATA_STORE["df"] = df_cleaned  # Mise √† jour du DataFrame stock√©.
    after = len(df_cleaned)  # Nombre de lignes apr√®s suppression.
    return {"removed": before - after, "remaining_rows": after}


# =============================================================================
# üóë SUPPRESSION DE COLONNES
# =============================================================================
@app.post("/drop-columns/")
def drop_columns(request: ColumnDropRequest):
    """
    Endpoint pour supprimer des colonnes sp√©cifiques du DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. V√©rifie que les colonnes demand√©es existent dans le DataFrame.
    3. Supprime les colonnes sp√©cifi√©es.
    4. Met √† jour DATA_STORE et retourne un message de confirmation.
    
    Args:
        request (ColumnDropRequest): Mod√®le contenant la liste des colonnes √† supprimer.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # V√©rifie que toutes les colonnes √† supprimer existent.
    missing = [col for col in request.columns if col not in df.columns]
    if missing:
        return JSONResponse(status_code=400, content={"error": f"Colonnes introuvables : {missing}"})
    # Suppression des colonnes sp√©cifi√©es.
    df.drop(columns=request.columns, inplace=True)
    DATA_STORE["df"] = df
    return {"message": f"{len(request.columns)} colonnes supprim√©es avec succ√®s."}


# =============================================================================
# üìÅ EXPORT DU CSV NETTOY√â
# =============================================================================
@app.get("/export/")
def export_csv():
    """
    Endpoint pour exporter le DataFrame nettoy√© au format CSV.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Sauvegarde le DataFrame dans un fichier CSV dans le dossier UPLOAD_DIR.
    3. Retourne le fichier CSV via FileResponse pour permettre son t√©l√©chargement.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # D√©finition du chemin pour sauvegarder le fichier CSV.
    path = os.path.join(UPLOAD_DIR, "data_cleaned.csv")
    df.to_csv(path, index=False)
    return FileResponse(path, media_type="text/csv", filename="data_cleaned.csv")


# =============================================================================
# üìà EXPORT D‚ÄôUN RAPPORT SIMPLE (DESCRIBE)
# =============================================================================
@app.get("/report/")
def generate_report():
    """
    Endpoint pour g√©n√©rer un rapport simple des statistiques descriptives du DataFrame.
    
    Processus :
    1. V√©rifie que le DataFrame est charg√©.
    2. Calcule un r√©sum√© statistique √† l'aide de la m√©thode describe(), le transpose.
    3. Exporte le r√©sum√© au format CSV dans le dossier UPLOAD_DIR.
    4. Retourne le fichier CSV du rapport via FileResponse pour t√©l√©chargement.
    """
    df = DATA_STORE.get("df")
    if df is None:
        return JSONResponse(status_code=404, content={"error": "Aucun fichier charg√©."})
    # D√©finition du chemin pour sauvegarder le rapport CSV.
    path = os.path.join(UPLOAD_DIR, "eda_report.csv")
    df.describe(include="all").transpose().to_csv(path)
    return FileResponse(path, media_type="text/csv", filename="eda_report.csv")
